{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d1772f-92fe-4aac-bc5c-7c164dd06662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alee/Desktop/projects/westford14.github.io/blog_posts/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b1b58-5158-4964-aa0e-244e8650ebbe",
   "metadata": {},
   "source": [
    "## Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5087e3d7-b0e1-4cb0-acb3-f477beb280f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x30c93d250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3947ba4-3c60-49a3-8980-1d734bfb1417",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a61091a-3370-46fa-ba68-a6f53dde3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"{YOUR_DATA_PATH}/sentence_level_data.csv\",\n",
    "    index_col=[0],\n",
    "    storage_options={\n",
    "        \"key\": \"REDACTED\",\n",
    "        \"secret\": \"REDACTED\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aae34f-06cf-4636-bc39-ff9699e6036a",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5493dec5-68fc-47ae-a1dc-c816058d5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Clean the text.\n",
    "\n",
    "    :param s: (str)\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    return s.lower().translate(s.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "df[\"cleaned_setence\"] = df[\"sentence\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4a8100-2db4-4df5-904e-c80e669d7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8858add-758f-4bfe-836f-3e6e17ea019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(s: str, lemmer: WordNetLemmatizer) -> str:\n",
    "    \"\"\"Lemmatize the text.\n",
    "\n",
    "    :param s: (str)\n",
    "    :param stemmer: (PorterStemmer)\n",
    "    :return: (str)\n",
    "    \"\"\"\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e743fa2-1fc3-4274-bb00-2c488e3011fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text\"] = df[\"cleaned_setence\"].apply(lambda x: lemmatize_text(x, lemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ffbec9-3f50-495b-9f2b-91c7a80b82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(df[\"lemmatized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b517f702-910d-4b68-87e5-e9fe5955b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPTDataset(Dataset):\n",
    "    def __init__(self, x_tfidf: list, y: int) -> None:\n",
    "        self.x_tfidf = x_tfidf\n",
    "        self.y = y\n",
    "        \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x_tfidf)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        return self.x_tfidf[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d542df0-b290-4681-a2b4-4a171a5ac09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_dataset = ChatGPTDataset(x_tfidf.toarray(), y=df[\"class\"].tolist())\n",
    "train_indices, test_indices = train_test_split(\n",
    "    list(range(0, len(chatgpt_dataset))), test_size=0.2, random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8480b8-205f-460f-a159-55f0c571ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d6cdc5-de28-4c8a-bf2e-57ae489b8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    chatgpt_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    chatgpt_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=test_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27857a69-2610-4e8c-9c1c-e7a9306efe64",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "055a2901-99e1-4204-8397-ff6a921aab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetBayes(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.pred = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.to(torch.float)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.pred(x).squeeze())\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dd0b500-00a6-46f5-bc11-afba18a1853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeller(x, y):\n",
    "    fc1w_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc1.weight),\n",
    "        scale=torch.ones_like(model.fc1.weight)\n",
    "    )\n",
    "    fc1b_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc1.bias),\n",
    "        scale=torch.ones_like(model.fc1.bias)\n",
    "    )\n",
    "\n",
    "    fc2w_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc2.weight),\n",
    "        scale=torch.ones_like(model.fc2.weight)\n",
    "    )\n",
    "    fc2b_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc2.bias),\n",
    "        scale=torch.ones_like(model.fc2.bias)\n",
    "    )\n",
    "\n",
    "    fc3w_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc3.weight),\n",
    "        scale=torch.ones_like(model.fc3.weight)\n",
    "    )\n",
    "    fc3b_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.fc3.bias),\n",
    "        scale=torch.ones_like(model.fc3.bias)\n",
    "    )\n",
    "    \n",
    "    predw_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.pred.weight),\n",
    "        scale=torch.ones_like(model.pred.weight)\n",
    "    )\n",
    "    predb_prior = pyro.distributions.Normal(\n",
    "        loc=torch.zeros_like(model.pred.bias),\n",
    "        scale=torch.ones_like(model.pred.bias)\n",
    "    )\n",
    "    \n",
    "    priors = {\n",
    "        'fc1.weight': fc1w_prior,\n",
    "        'fc1.bias': fc1b_prior,\n",
    "        'fc2.weight': fc2w_prior,\n",
    "        'fc2.bias': fc2b_prior,\n",
    "        'fc3.weight': fc3w_prior,\n",
    "        'fc3.bias': fc3b_prior,\n",
    "        'pred.weight': predw_prior,\n",
    "        'pred.bias': predb_prior\n",
    "    }\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    lifted_reg_model = lifted_module()\n",
    "\n",
    "    lhat = nn.LogSoftmax(dim=1)(lifted_reg_model(x))\n",
    "\n",
    "    pyro.sample(\"obs\", pyro.distributions.torch.Categorical(logits=lhat), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fc451b0-c187-4c01-b42f-824bf761eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x, y):\n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(model.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(model.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = nn.Softplus()(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = pyro.distributions.Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(model.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(model.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = nn.Softplus()(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = pyro.distributions.Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    # Second layer\n",
    "    fc2w_mu = torch.randn_like(model.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(model.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = nn.Softplus()(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_prior = pyro.distributions.Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    # Second layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(model.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(model.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = nn.Softplus()(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_prior = pyro.distributions.Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "    # Third layer\n",
    "    fc3w_mu = torch.randn_like(model.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(model.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = nn.Softplus()(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3w_prior = pyro.distributions.Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param)\n",
    "    # Second layer bias distribution priors\n",
    "    fc3b_mu = torch.randn_like(model.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(model.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = nn.Softplus()(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3b_prior = pyro.distributions.Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "    # Output layer weight distribution priors\n",
    "    predw_mu = torch.randn_like(model.pred.weight)\n",
    "    predw_sigma = torch.randn_like(model.pred.weight)\n",
    "    predw_mu_param = pyro.param(\"outw_mu\", predw_mu)\n",
    "    predw_sigma_param = nn.Softplus()(pyro.param(\"outw_sigma\", predw_sigma))\n",
    "    predw_prior = pyro.distributions.Normal(loc=predw_mu_param, scale=predw_sigma_param).independent(1)\n",
    "    # Output layer bias distribution priors\n",
    "    predb_mu = torch.randn_like(model.pred.bias)\n",
    "    predb_sigma = torch.randn_like(model.pred.bias)\n",
    "    predb_mu_param = pyro.param(\"predb_mu\", predb_mu)\n",
    "    predb_sigma_param = nn.Softplus()(pyro.param(\"predb_sigma\", predb_sigma))\n",
    "    predb_prior = pyro.distributions.Normal(loc=predb_mu_param, scale=predb_sigma_param)\n",
    "    priors = {\n",
    "        'fc1.weight': fc1w_prior,\n",
    "        'fc1.bias': fc1b_prior,\n",
    "        'fc2.weight': fc2w_prior,\n",
    "        'fc2.bias': fc2b_prior,\n",
    "        'fc3.weight': fc3w_prior,\n",
    "        'fc3.bias': fc3b_prior,\n",
    "        'pred.weight': predw_prior,\n",
    "        'pred.bias': predb_prior\n",
    "    }\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", model, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bec1904-9e40-449b-8f27-0acdfb368b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNetBayes(input_shape=chatgpt_dataset.x_tfidf.shape[1])\n",
    "adam_args = {\"lr\": 0.005}\n",
    "optimizer = Adam(adam_args)\n",
    "elbo = Trace_ELBO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e4fa3aa-6e8d-4911-b919-ead96ef2ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(modeller, guide, optimizer, loss=elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e910d5b-18c5-476b-ab53-e488ec4f8f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/20 [00:00<?, ?it/s]/var/folders/nm/8hbqz6514gz54kp696nvk55c0000gn/T/ipykernel_27624/1064750372.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.pred(x).squeeze())\n",
      "  5%|██████                                                                                                                   | 1/20 [01:36<30:27, 96.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss  166292.18103979586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████                                                                                                             | 2/20 [03:12<28:53, 96.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Loss  83316.8292290688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████▏                                                                                                      | 3/20 [04:50<27:30, 97.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2  Loss  50009.50883185845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████▏                                                                                                | 4/20 [06:30<26:11, 98.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3  Loss  33838.90256885314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████▎                                                                                          | 5/20 [08:08<24:33, 98.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4  Loss  24528.832419462615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████▎                                                                                    | 6/20 [09:46<22:51, 97.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5  Loss  18496.96980904455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████████████▎                                                                              | 7/20 [11:22<21:08, 97.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6  Loss  14249.23374544545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████▍                                                                        | 8/20 [12:59<19:28, 97.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7  Loss  11236.658984638718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████████████████████████▍                                                                  | 9/20 [14:36<17:49, 97.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8  Loss  8986.921254552528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████                                                            | 10/20 [16:13<16:10, 97.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9  Loss  7286.71979393276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████████████████████████████                                                      | 11/20 [17:51<14:35, 97.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10  Loss  6027.378449516034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████                                                | 12/20 [19:27<12:55, 96.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11  Loss  5050.498555700179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████████████████████████████████                                          | 13/20 [20:59<11:08, 95.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12  Loss  4277.3238145544765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████████                                    | 14/20 [22:35<09:33, 95.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13  Loss  3668.331450677142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████                              | 15/20 [24:11<07:58, 95.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14  Loss  3189.620796243723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████                        | 16/20 [25:48<06:23, 95.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15  Loss  2789.40002029868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████                  | 17/20 [27:23<04:47, 95.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16  Loss  2466.610961653227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 18/20 [28:59<03:11, 95.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17  Loss  2204.1753551807246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 19/20 [30:35<01:35, 95.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18  Loss  1986.2088803329823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [32:11<00:00, 96.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19  Loss  1795.8730080969842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 20\n",
    "loss = 0\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "for j in tqdm(range(num_iterations)):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        loss += svi.step(data[0], data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81bae8fd-008e-4043-9d48-fa34bff42f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nm/8hbqz6514gz54kp696nvk55c0000gn/T/ipykernel_27624/1064750372.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.pred(x).squeeze())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50 %\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(2)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return torch.argmax(mean, dim=1)\n",
    "\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(validation_loader):\n",
    "    text, labels = data\n",
    "    predicted = predict(text)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d6209-3600-47f6-a55c-5308b1975db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
